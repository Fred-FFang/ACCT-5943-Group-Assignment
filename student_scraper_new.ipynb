{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lxml\n",
    "# %pip install html5lib\n",
    "# %pip install selenium\n",
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install bs4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_companies(group_number): #get the group number from your lecturer\n",
    "    # File to store selected companies\n",
    "    selection_file = f'selected_companies_group_{group_number}.txt'\n",
    "    \n",
    "    # Check if we have already selected companies for this group\n",
    "    if os.path.exists(selection_file):\n",
    "        with open(selection_file, 'r') as f:\n",
    "            return [line.strip().split(',') for line in f.readlines()]\n",
    "    \n",
    "    # If not, perform the selection\n",
    "    rng = np.random.default_rng(group_number)  # Use group number as seed\n",
    "    companies = pd.read_csv(\"filtered_sp-500.csv\")\n",
    "    industries = list(set(companies['Sector']))\n",
    "    selected_industries = rng.choice(industries, size=min(10, len(industries)), replace=False) #select 10 industries\n",
    "\n",
    "    selected_companies = []\n",
    "    for industry in selected_industries:\n",
    "        industry_companies = companies[companies['Sector'] == industry].values.tolist()\n",
    "        selected_companies.extend(rng.choice(industry_companies, size=min(20, len(industry_companies)), replace=False)) #select 20 companies in each industry\n",
    "\n",
    "    # Convert numpy arrays to lists\n",
    "    selected_companies = [company.tolist() if isinstance(company, np.ndarray) else company for company in selected_companies]\n",
    "\n",
    "    # Save the selection\n",
    "    with open(selection_file, 'w') as f:\n",
    "        for company in selected_companies:\n",
    "            f.write(','.join(map(str, company)) + '\\n')\n",
    "\n",
    "    return selected_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_company_data(selected_companies, url):\n",
    "\n",
    "    # Setup Selenium WebDriver (you may need to adjust this based on your browser)\n",
    "    # You can see this scraping process in action\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    service = Service('/opt/homebrew/bin/chromedriver') \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Runs Chrome in headless mode (no UI)\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options) # or webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#dataTable tbody tr\"))\n",
    "        )\n",
    "\n",
    "        all_data = []\n",
    "        company_codes = [company[0] for company in selected_companies]  # Extract company codes\n",
    "\n",
    "        while True:\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Extract headers from the appropriate div\n",
    "            header_table = soup.find('div', {'class': 'dataTables_scrollHead'})\n",
    "            if header_table is None:\n",
    "                print(\"Table not found. Please check the HTML structure.\")\n",
    "                break\n",
    "            headers = [header.text.strip() for header in header_table.find_all('th')]\n",
    "            \n",
    "            # Extract rows from the table body\n",
    "            table_body = soup.find(\"table\", id='dataTable')\n",
    "            rows = []\n",
    "            for row in table_body.find_all('tr'):\n",
    "                cols = [ele.text.strip() for ele in row.find_all('td')]\n",
    "                if len(cols) > 0 and cols[0].split('-')[0] in company_codes:\n",
    "                    rows.append(cols)\n",
    "                \n",
    "            # Convert to DataFrame and append if data matches selected companies\n",
    "            if rows:\n",
    "                df = pd.DataFrame(rows, columns=headers)\n",
    "                all_data.append(df)\n",
    "\n",
    "            # Check if there's a next page\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, \"#dataTable_next:not(.disabled)\")\n",
    "                next_button.click()\n",
    "                time.sleep(1)  # Wait for the page to load\n",
    "            except:\n",
    "                break  # No more pages\n",
    "\n",
    "        # Combine all data\n",
    "        if all_data:\n",
    "            final_data = pd.concat(all_data, ignore_index=True)\n",
    "        else:\n",
    "            final_data = pd.DataFrame()  # Return empty DataFrame if no data is found\n",
    "\n",
    "        return final_data\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABT (Health Care)\n",
      "ADM (Consumer Staples)\n",
      "AEE (Utilities)\n",
      "AEP (Utilities)\n",
      "AES (Utilities)\n",
      "AKAM (Information Technology)\n",
      "ALB (Materials)\n",
      "ALGN (Health Care)\n",
      "ALLE (Industrials)\n",
      "AMGN (Health Care)\n",
      "AMT (Real Estate)\n",
      "AMZN (Consumer Discretionary)\n",
      "ANET (Information Technology)\n",
      "ANSS (Information Technology)\n",
      "APH (Information Technology)\n",
      "APTV (Consumer Discretionary)\n",
      "AVB (Real Estate)\n",
      "AVY (Materials)\n",
      "AWK (Utilities)\n",
      "AXP (Financials)\n",
      "BAC (Financials)\n",
      "BALL (Materials)\n",
      "BIO (Health Care)\n",
      "BK (Financials)\n",
      "BKNG (Consumer Discretionary)\n",
      "BLK (Financials)\n",
      "BMY (Health Care)\n",
      "BSX (Health Care)\n",
      "BWA (Consumer Discretionary)\n",
      "BXP (Real Estate)\n",
      "CB (Financials)\n",
      "CCI (Real Estate)\n",
      "CDNS (Information Technology)\n",
      "CDW (Information Technology)\n",
      "CE (Materials)\n",
      "CF (Materials)\n",
      "CHD (Consumer Staples)\n",
      "CHRW (Industrials)\n",
      "CHTR (Communication Services)\n",
      "CL (Consumer Staples)\n",
      "CMCSA (Communication Services)\n",
      "CME (Financials)\n",
      "CMG (Consumer Discretionary)\n",
      "CNC (Health Care)\n",
      "CPT (Real Estate)\n",
      "CSGP (Real Estate)\n",
      "CTSH (Information Technology)\n",
      "CTVA (Materials)\n",
      "CVS (Health Care)\n",
      "DAL (Industrials)\n",
      "DAY (Industrials)\n",
      "DHR (Health Care)\n",
      "DLR (Real Estate)\n",
      "DOC (Real Estate)\n",
      "DOW (Materials)\n",
      "DTE (Utilities)\n",
      "DUK (Utilities)\n",
      "EBAY (Consumer Discretionary)\n",
      "ECL (Materials)\n",
      "ED (Utilities)\n",
      "EG (Financials)\n",
      "EIX (Utilities)\n",
      "ENPH (Information Technology)\n",
      "EPAM (Information Technology)\n",
      "EQR (Real Estate)\n",
      "ES (Utilities)\n",
      "ESS (Real Estate)\n",
      "ETN (Industrials)\n",
      "ETSY (Consumer Discretionary)\n",
      "EW (Health Care)\n",
      "EXC (Utilities)\n",
      "EXPE (Consumer Discretionary)\n",
      "EXR (Real Estate)\n",
      "FAST (Industrials)\n",
      "FCX (Materials)\n",
      "FIS (Financials)\n",
      "FMC (Materials)\n",
      "FTNT (Information Technology)\n",
      "GILD (Health Care)\n",
      "GL (Financials)\n",
      "GOOG (Communication Services)\n",
      "GOOGL (Communication Services)\n",
      "GRMN (Consumer Discretionary)\n",
      "GWW (Industrials)\n",
      "HAS (Consumer Discretionary)\n",
      "HII (Industrials)\n",
      "HLT (Consumer Discretionary)\n",
      "HON (Industrials)\n",
      "HSY (Consumer Staples)\n",
      "ICE (Financials)\n",
      "IEX (Industrials)\n",
      "INTC (Information Technology)\n",
      "IP (Materials)\n",
      "IPG (Communication Services)\n",
      "IVZ (Financials)\n",
      "JNPR (Information Technology)\n",
      "K (Consumer Staples)\n",
      "KDP (Consumer Staples)\n",
      "KEY (Financials)\n",
      "KHC (Consumer Staples)\n",
      "KIM (Real Estate)\n",
      "KMB (Consumer Staples)\n",
      "KO (Consumer Staples)\n",
      "LDOS (Industrials)\n",
      "LHX (Industrials)\n",
      "LLY (Health Care)\n",
      "LNT (Utilities)\n",
      "LVS (Consumer Discretionary)\n",
      "LYB (Materials)\n",
      "LYV (Communication Services)\n",
      "MDLZ (Consumer Staples)\n",
      "META (Communication Services)\n",
      "MGM (Consumer Discretionary)\n",
      "MLM (Materials)\n",
      "MMC (Financials)\n",
      "MNST (Consumer Staples)\n",
      "MO (Consumer Staples)\n",
      "MOH (Health Care)\n",
      "MOS (Materials)\n",
      "MPWR (Information Technology)\n",
      "MS (Financials)\n",
      "MSI (Information Technology)\n",
      "MTCH (Communication Services)\n",
      "MTD (Health Care)\n",
      "NCLH (Consumer Discretionary)\n",
      "NDAQ (Financials)\n",
      "NEE (Utilities)\n",
      "NEM (Materials)\n",
      "NFLX (Communication Services)\n",
      "NOW (Information Technology)\n",
      "NRG (Utilities)\n",
      "NTRS (Financials)\n",
      "NXPI (Information Technology)\n",
      "OMC (Communication Services)\n",
      "PARA (Communication Services)\n",
      "PAYC (Industrials)\n",
      "PCG (Utilities)\n",
      "PEG (Utilities)\n",
      "PEP (Consumer Staples)\n",
      "PHM (Consumer Discretionary)\n",
      "PKG (Materials)\n",
      "PLD (Real Estate)\n",
      "PM (Consumer Staples)\n",
      "PNC (Financials)\n",
      "PODD (Health Care)\n",
      "POOL (Consumer Discretionary)\n",
      "PPG (Materials)\n",
      "PPL (Utilities)\n",
      "PRU (Financials)\n",
      "PSA (Real Estate)\n",
      "PWR (Industrials)\n",
      "RCL (Consumer Discretionary)\n",
      "REG (Real Estate)\n",
      "REGN (Health Care)\n",
      "ROL (Industrials)\n",
      "RVTY (Health Care)\n",
      "SHW (Materials)\n",
      "SNA (Industrials)\n",
      "SO (Utilities)\n",
      "SPG (Real Estate)\n",
      "SRE (Utilities)\n",
      "STLD (Materials)\n",
      "T (Communication Services)\n",
      "TAP (Consumer Staples)\n",
      "TDY (Information Technology)\n",
      "TMUS (Communication Services)\n",
      "TSCO (Consumer Discretionary)\n",
      "TT (Industrials)\n",
      "TXN (Information Technology)\n",
      "TYL (Information Technology)\n",
      "UAL (Industrials)\n",
      "UPS (Industrials)\n",
      "USB (Financials)\n",
      "VICI (Real Estate)\n",
      "VMC (Materials)\n",
      "VST (Utilities)\n",
      "VTR (Real Estate)\n",
      "VZ (Communication Services)\n",
      "WAT (Health Care)\n",
      "WEC (Utilities)\n",
      "WELL (Real Estate)\n",
      "WST (Health Care)\n",
      "WTW (Financials)\n",
      "WY (Real Estate)\n",
      "WYNN (Consumer Discretionary)\n",
      "XYL (Industrials)\n",
      "YUM (Consumer Discretionary)\n",
      "ZBH (Health Care)\n",
      "ZBRA (Information Technology)\n",
      "Data Scraping finished, please check your data in the csv file\n",
      "Data saved to group_53_data.csv\n"
     ]
    }
   ],
   "source": [
    "def main(group_number, url):\n",
    "    selected_companies = select_companies(group_number)\n",
    "    company_data = scrape_company_data(selected_companies, url)\n",
    "\n",
    "    sorted_companies = sorted(selected_companies, key=lambda x: x[0])\n",
    "    for company in sorted_companies:\n",
    "        print(f\"{company[0]} ({company[2]})\")\n",
    "\n",
    "    print(\"Data Scraping finished, please check your data in the csv file\")\n",
    "    company_data.to_csv(f\"group_{group_number}_data.csv\", index=False)\n",
    "    print(f\"Data saved to group_{group_number}_data.csv\")\n",
    "\n",
    "url = \"https://unsw-yahoo-finance.github.io/ACCT5943/\" # Replace with the actual website\n",
    "# TODO: update the group_number to your assigned group number, then click run-all\n",
    "group_number = 53\n",
    "main(group_number, url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
